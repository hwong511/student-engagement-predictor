---
title: 'Data Merging Notebook for BROMP and Caliper'
author: "Carnegie Learning Capstone Group"
date: '`r lubridate::today()`'
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

## Setup

```{r}
#| warning: false
library(readr)
library(dplyr)
library(lubridate)
library(tidyr)
library(janitor)
library(fuzzyjoin) 
library(data.table)

path_data <- "data"
```

## Data Merging

### Loading in the Data

```{r}
d1 <- read_csv(here::here(path_data, 'BROMP-clean.csv')) %>%  janitor::clean_names()
d2 <- read_csv(here::here(path_data, 'caliper-clean.csv')) %>% janitor::clean_names()
```

For the sake of ease, I'm gonna rename both of the student IDs in the two spreadsheet to the same name.

```{r}
d1 <- d1 %>% rename(student_id = anon_student_id)
d2 <- d2 %>% rename(student_id = student_id_anon)
```

### Cleaning & Deciding Merge Approach

Should we wish to merge the two dataset, we need to determine the best approach in doing so. For the data at hand, we can see from the code chunk below that they have 163 students in common. That being said, the caliper dataset has many more rows (n=15386) compared to the BROMP dataset (n=4037), meaning that we cannot merge the two datasets exactly.

```{r}
length(intersect(d1$student_id, d2$student_id)) # Number of student_ids in common
length(unique(d1$student_id)) # Number of students for the BROMP dataset
length(unique(d2$student_id)) # Number of students for the caliper dataset
```

The current approach I've decided on is **merging via time windows**. That is, for each BROMP observation, we will find all Caliper events within ± N minutes (e.g., ± 5 mins) for the same student. The pros of this approach are:

-   Links behaviors to multiple digital activities happening around the same time
-   Preserves all data from both datasets
-   Flexible (we can adjust window size based on research needs)
-   Enables analysis of behavioral patterns around specific digital events
-   Handles frequency differences (which as stated before, is the case for our datasets)
-   One behavior can be linked to multiple digital activities = more realistic

That being said, the first con that jumps to my mind is that the results would be sensitive to the arbitrarily chosen time window. Furthermore, the merged dataset would be MASSIVE, and leads to more complex data analysis. Nonetheless, the pros outweigh the cons, as this approach would allow sequence analysis and temporal pattern detection (which might be helpful for some of our RQs!). Additionally, with n=163, we should have pretty good statistical power even with all the added complexity.

Another potential merging appraoch would be **merging via nearest neighbors**. That is, for each BROMP observation, we will find the single closest Caliper event in time for the same student. The pros of this approach are:

-   Much simpler and cleaner dataset (it would have the same number of rows as BROMP dataset)
-   Easy to analyze
-   Clear interpretation, as each behavior is linked to one specific digital event

That being said, this approach leads to information loss, as it ignores other potentially relevant events happening "nearby". On a similar note, it might miss more relevant events that are slightly further away, meaning it would be more sensitive to outliers, leading to inaccurate/misleading associations. Still, this would be good for simple correlation studies that focus on immediate behavior-event relationships.

ANYWAYS, let me clean the timestamps real quick...

```{r}
d1 <- d1 %>%
  mutate(
    time_utc = with_tz(ntp_time, "UTC")
  ) %>%
  filter(!is.na(time_utc), !is.na(student_id))
```

```{r}
d2 <- d2 %>%
  mutate(
    time_utc = ymd_hms(event_time)
  ) %>%
  filter(!is.na(time_utc), !is.na(student_id))
```

### Merging via Time Windows

One thing I changed was to make the merging method backward-looking - My old method includes Caliper events that happened AFTER the BROMP observation. For my goal of prediction, I feel it'd be problematic to use "future" events to predict "past" behaviors. Therefore, as seen in the code, I've decided to merge events using symmetric window with multi-window features.

Note:
For `time_diff`: 
- Positive = Caliper event happened AFTER BROMP observation\
- Negative = Caliper event happened BEFORE BROMP observation

The following code chunk gives us a merged dataset with SYMMETRIC features. We shouldn't use this to train the model, but if we want to, we can compare our final model with it to examine the differences.

```{r}
d_merged <- d1 %>%
  fuzzy_left_join(d2, 
                  by = c("student_id" = "student_id", "time_utc" = "time_utc"),
                  match_fun = list(`==`, function(x, y) {
                    abs(difftime(x, y, units = "secs")) <= 90
                  })) %>%
  rename(bromp_time = time_utc.x, caliper_time = time_utc.y) %>%
  mutate(student_id = student_id.x,
         time_diff = as.numeric(difftime(caliper_time, bromp_time, units = "secs")))
```

For d_features, which we will use to train the models, we will create feature separately for before and after.

```{r}
d_features <- d_merged %>%
  group_by(student_id, bromp_time, behavior, affect, class) %>%
  summarise(
    # BACKWARD window features (for prediction)
    events_back_30s = sum(!is.na(caliper_time) & time_diff >= -30 & time_diff < 0),
    events_back_60s = sum(!is.na(caliper_time) & time_diff >= -60 & time_diff < 0),
    events_back_90s = sum(!is.na(caliper_time) & time_diff >= -90 & time_diff < 0),
    
    pauses_back_60s = sum(!is.na(caliper_time) & time_diff >= -60 & time_diff < 0 & 
                          action == "Paused", na.rm = TRUE),
    assessment_back_60s = sum(!is.na(caliper_time) & time_diff >= -60 & time_diff < 0 & 
                              event_type == "AssessmentEvent", na.rm = TRUE),
    
    # FORWARD window features (for understanding context)
    events_forward_30s = sum(!is.na(caliper_time) & time_diff > 0 & time_diff <= 30),
    events_forward_60s = sum(!is.na(caliper_time) & time_diff > 0 & time_diff <= 60),
    
    # SYMMETRIC window features (for full context)
    events_symmetric_60s = sum(!is.na(caliper_time) & abs(time_diff) <= 60),
    events_symmetric_90s = sum(!is.na(caliper_time) & abs(time_diff) <= 90),
    
    pauses_symmetric_60s = sum(!is.na(caliper_time) & abs(time_diff) <= 60 & 
                               action == "Paused", na.rm = TRUE),
    
    # Most recent event (backward only)
    time_since_last_event = ifelse(
      any(!is.na(caliper_time) & time_diff < 0),
      abs(max(time_diff[!is.na(caliper_time) & time_diff < 0])),
      600
    ),
    
    most_recent_event_type = ifelse(
      any(!is.na(caliper_time) & time_diff < 0),
      event_type[which.max(time_diff[!is.na(caliper_time) & time_diff < 0])],
      "NO_ACTIVITY"
    ),
    
    .groups = 'drop'
  )
```

```{r}
write.csv(d_merged, "+-90sec_merged_data.csv")
```

```{r}
write.csv(d_features, "prediction_features.csv")
```









## Validating the merge (just in case)

NOTE: I got lazy. These are all AI checks. Please let me know if you notice something problematic in my approach or the final dataset.

### Basic Counts and Coverage

```{r}
cat("=== BASIC VALIDATION ===\n")

# Original dataset sizes
cat("Original BROMP observations:", nrow(d1), "\n")
cat("Original Caliper events:", nrow(d2), "\n")
cat("Merged dataset size:", nrow(d_merged), "\n")
cat("Expansion ratio:", round(nrow(d_merged) / nrow(d1), 2), "x\n\n")

# Student coverage
students_d1 <- unique(d1$student_id)
students_d2 <- unique(d2$student_id)
students_merged <- unique(d_merged$student_id)

cat("Students in BROMP:", length(students_d1), "\n")
cat("Students in Caliper:", length(students_d2), "\n")
cat("Students in merged data:", length(students_merged), "\n")
cat("% BROMP students in merge:", round(100 * length(students_merged) / length(students_d1), 1), "%\n\n")

# Check for missing students
missing_students <- setdiff(students_d1, students_merged)
if (length(missing_students) > 0) {
  cat("WARNING: Students from BROMP missing in merge:", length(missing_students), "\n")
  cat("Missing student IDs:", head(missing_students, 5), "\n\n")
}
```

### Time Difference Validation

```{r}
cat("=== TIME ALIGNMENT VALIDATION ===\n")

# Time difference statistics
time_diff_stats <- d_merged %>%
  summarise(
    mean_diff = round(mean(abs(time_diff)), 2),
    median_diff = round(median(abs(time_diff)), 2),
    q25_diff = round(quantile(abs(time_diff), 0.25), 2),
    q75_diff = round(quantile(abs(time_diff), 0.75), 2),
    max_diff = round(max(abs(time_diff)), 2),
    within_1min = round(100 * mean(abs(time_diff) <= 60), 1),
    within_2min = round(100 * mean(abs(time_diff) <= 120), 1),
    within_5min = round(100 * mean(abs(time_diff) <= 300), 1)
  )

print(time_diff_stats)
cat("\n")

# Check for systematic time bias
pos_neg_bias <- d_merged %>%
  summarise(
    positive_diffs = sum(time_diff > 0),
    negative_diffs = sum(time_diff < 0),
    mean_signed_diff = round(mean(time_diff), 2)
  )

cat("Positive time diffs (Caliper after BROMP):", pos_neg_bias$positive_diffs, "\n")
cat("Negative time diffs (Caliper before BROMP):", pos_neg_bias$negative_diffs, "\n")
cat("Mean signed difference:", pos_neg_bias$mean_signed_diff, "seconds\n")

if (abs(pos_neg_bias$mean_signed_diff) > 30) {
  cat("WARNING: Systematic time bias detected!\n")
}
cat("\n")
```

### Temporal Distribution Check

```{r}
cat("=== TEMPORAL DISTRIBUTION CHECK ===\n")

# Check if merge preserves temporal patterns
bromp_by_hour <- d1 %>%
  mutate(hour = hour(time_utc)) %>%
  count(hour, name = "bromp_count")

merged_by_hour <- d_merged %>%
  mutate(hour = hour(bromp_time)) %>%
  count(hour, name = "merged_count")

temporal_check <- bromp_by_hour %>%
  left_join(merged_by_hour, by = "hour") %>%
  mutate(
    coverage_pct = round(100 * (merged_count / bromp_count), 1)
  )

cat("Hourly coverage check:\n")
print(temporal_check)
cat("\n")

# Check for hours with poor coverage
poor_coverage <- temporal_check %>%
  filter(coverage_pct < 50)

if (nrow(poor_coverage) > 0) {
  cat("WARNING: Poor coverage in these hours:\n")
  print(poor_coverage)
  cat("\n")
}
```

### Student-Level Validation

```{r}
cat("=== STUDENT-LEVEL VALIDATION ===\n")

# Check merge success by student
student_validation <- d1 %>%
  group_by(student_id) %>%
  summarise(bromp_obs = n(), .groups = "drop") %>%
  left_join(
    d_merged %>%
      group_by(student_id) %>%
      summarise(
        merged_records = n(),
        unique_bromp_times = n_distinct(bromp_time),
        avg_events_per_obs = round(n() / n_distinct(bromp_time), 2),
        .groups = "drop"
      ),
    by = "student_id"
  ) %>%
  mutate(
    coverage_pct = round(100 * unique_bromp_times / bromp_obs, 1),
    merge_success = !is.na(merged_records)
  )

# Summary statistics
cat("Student merge success rate:", 
    round(100 * mean(student_validation$merge_success), 1), "%\n")

coverage_stats <- student_validation %>%
  filter(merge_success) %>%
  summarise(
    mean_coverage = round(mean(coverage_pct), 1),
    median_coverage = round(median(coverage_pct), 1),
    min_coverage = min(coverage_pct)
  )

cat("Mean student coverage:", coverage_stats$mean_coverage, "%\n")
cat("Median student coverage:", coverage_stats$median_coverage, "%\n")
cat("Minimum student coverage:", coverage_stats$min_coverage, "%\n\n")

# Students with poor merge results
poor_merge <- student_validation %>%
  filter(merge_success & coverage_pct < 25)

if (nrow(poor_merge) > 0) {
  cat("WARNING: Students with <25% coverage:\n")
  print(head(poor_merge))
  cat("\n")
}
```

### Content Validation

```{r}
cat("=== CONTENT VALIDATION ===\n")

# Check for reasonable behavior-event combinations
behavior_event_combos <- d_merged %>%
  count(behavior, event_type, sort = TRUE) %>%
  head(10)

cat("Top 10 Behavior-Event combinations:\n")
print(behavior_event_combos)
cat("\n")

# Check for unexpected patterns
unexpected_patterns <- d_merged %>%
  filter(
    (behavior == "OFF TASK" & event_type %in% c("AssessmentEvent", "QuestionEvent")) |
    (behavior == "ON TASK" & event_type == "NavigationEvent")
  ) %>%
  nrow()

cat("Potentially unexpected behavior-event combinations:", unexpected_patterns, "\n")
cat("(This might be normal - just flagging for review)\n\n")
```

### Data Quality Check

```{r}
cat("=== DATA QUALITY CHECKS ===\n")

# Check for duplicates
duplicates <- d_merged %>%
  group_by(student_id, bromp_time, caliper_event_id) %>%
  filter(n() > 1) %>%
  nrow()

cat("Duplicate records:", duplicates, "\n")

# Check for missing critical values
missing_check <- d_merged %>%
  summarise(
    missing_behavior = sum(is.na(behavior)),
    missing_event_type = sum(is.na(event_type)),
    missing_time_diff = sum(is.na(time_diff))
  )

cat("Missing behavior values:", missing_check$missing_behavior, "\n")
cat("Missing event_type values:", missing_check$missing_event_type, "\n")
cat("Missing time_diff values:", missing_check$missing_time_diff, "\n\n")
```

```{r}
beepr::beep()
```
